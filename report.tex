\documentclass[12pt]{article}

% фонтови и језик
\usepackage{fontspec}
\usepackage{polyglossia}
% Користимо Liberation фонтове који подржавају ћирилицу
\setmainfont{Liberation Serif}
\setsansfont{Liberation Sans}
\setmonofont{Liberation Mono}
\newfontfamily\cyrillicfont{Liberation Serif}
\newfontfamily\cyrillicfontsf{Liberation Sans}
\newfontfamily\cyrillicfonttt{Liberation Mono}
\setmainlanguage[Script=Cyrillic]{serbian}
\setotherlanguage{english}

% веб адресе и хиперлинкови
\usepackage{url}
\usepackage{hyperref}

% математички изрази
\usepackage{amsmath}
\usepackage{amssymb}

% графика
\usepackage{graphicx}
\graphicspath{{figures/}}

% маргине
\usepackage[left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}

% размак линија
\usepackage{setspace}

% табеле
\usepackage{booktabs}
\usepackage{array}

% float позиционирање
\usepackage{float}

% наслов и аутор
\title{Паралелизација N-body симулације:\\ Поређење CPU и GPU имплементација}
\author{Марко Петров}
\date{Фебруар 2026}


\begin{document}

% ============================================================================
% НАСЛОВНА СТРАНА
% ============================================================================
\begin{titlepage}
\centering
\vspace*{2cm}

{\Large\textbf{Универзитет у Новом Саду}}\\[0.3cm]
{\Large Факултет техничких наука}\\[0.3cm]
{\large Рачунарство и аутоматика}\\[2cm]

{\LARGE\textbf{Рачунарски системи високих перформанси}}\\[0.5cm]
{\large Семинарски рад}\\[2cm]

{\Huge\textbf{Паралелизација N-body симулације}}\\[0.5cm]
{\Large\textbf{Поређење CPU и GPU имплементација}}\\[3cm]

\begin{flushright}
{\large Студент: Марко Кубурић}\\
{\large Индекс: E2 62/2025}\\[1cm]
{\large Ментор: проф. др Вељко Петровић}
\end{flushright}

\vfill
{\large Нови Сад, фебруар 2026.}
\end{titlepage}

% ============================================================================
% АПСТРАКТ
% ============================================================================
\begin{abstract}
\doublespacing
Овај рад представља детаљну студију паралелизације директне N-body симулације са квадратном рачунском сложеношћу $O(N^2)$. Анализиране су четири варијанте CPU имплементације — секвенцијална несиметрична, секвенцијална симетрична (применом Њутновог трећег закона), OpenMP несиметрична и OpenMP симетрична (са атомским операцијама) — као и CUDA имплементација за GPU. 

Користи се наивна all-pairs формулација како би се изоловали архитектонски ефекти паралелизације, уместо алгоритамских оптимизација попут Barnes--Hut методе. Резултати показују да симетрична формулација убрзава секвенцијално извршавање за фактор 1,4×, али значајно успорава OpenMP имплементацију због контенције атомских операција. За финално поређење изабране су најбоља секвенцијална (симетрична), најбоља OpenMP (несиметрична) и CUDA имплементација.

Експерименти су спроведени на Intel Core i9-13900HX процесору са 32 логичке нити и NVIDIA GeForce RTX 4070 Laptop GPU. При $N = 12000$ честица, OpenMP имплементација постиже убрзање од 9,1× у односу на секвенцијалну, док CUDA остварује убрзање од 17,6×. GPU преузима доминацију над CPU за $N \geq 4000$ честица.
\end{abstract}
\pagebreak

% ============================================================================
% САДРЖАЈ
% ============================================================================
\tableofcontents
\pagebreak

% ============================================================================
% УВОД
% ============================================================================
\section{Увод}

Симулација N-тела (\textit{N-body simulation}) представља један од фундаменталних проблема у рачунарској физици и астрофизици. Проблем се састоји у симулацији кретања $N$ честица које интерагују међусобно путем гравитационих, електростатичких или других сила дугог домета. Примене обухватају симулацију галаксија, молекуларну динамику, плазма физику и анализу честичних система \cite{aarseth2003gravitational}.

Директна метода, позната и као \textit{all-pairs} или \textit{brute-force} метода, израчунава интеракцију сваког пара честица, што резултује квадратном рачунском сложеношћу $O(N^2)$ по временском кораку. Иако постоје ефикаснији алгоритми попут Barnes--Hut методе са сложеношћу $O(N \log N)$ \cite{barnes1986hierarchical} или Fast Multipole Method (FMM) са $O(N)$ \cite{greengard1987fast}, директна метода остаје релевантна за мање системе и као референтна тачка за валидацију.

У контексту рачунарских система високих перформанси (HPC), N-body симулација представља одличан тест случај за анализу архитектонских ефеката паралелизације. Овај рад се фокусира на \textbf{изоловање архитектонских ефеката} — утицај броја нити, меморијске хијерархије, кохеренције кеша и GPU архитектуре — уместо алгоритамских оптимизација. Из тог разлога користи се наивна all-pairs формулација.

Рад анализира две кључне формулације. Прва је \textbf{несиметрична формулација}, у којој свака честица израчунава силу од свих осталих честица, што даје укупно $N(N-1)$ интеракција по временском кораку. Друга је \textbf{симетрична формулација}, која користи Њутнов трећи закон ($\vec{F}_{ij} = -\vec{F}_{ji}$) да преполови број рачунања сила, али захтева ажурирање два акумулатора по интеракцији.

Циљ рада је систематско поређење имплементација на CPU (секвенцијално и OpenMP) и GPU (CUDA), уз детаљну анализу фактора који утичу на перформансе.


% ============================================================================
% ТЕОРИЈСКА ПОЗАДИНА
% ============================================================================
\section{Теоријска позадина}

\subsection{Физички модел}

N-body симулација моделује систем од $N$ честица са масама $m_i$, позицијама $\vec{r}_i$ и брзинама $\vec{v}_i$. Гравитациона сила између честица $i$ и $j$ дата је Њутновим законом гравитације:

\begin{equation}
\vec{F}_{ij} = G \frac{m_i m_j}{|\vec{r}_{ij}|^2 + \epsilon^2} \hat{r}_{ij}
\label{eq:gravity}
\end{equation}

где је $\vec{r}_{ij} = \vec{r}_j - \vec{r}_i$, $\hat{r}_{ij}$ јединични вектор, $G$ гравитациона константа, а $\epsilon$ параметар омекшавања (\textit{softening parameter}) који спречава сингуларитет при малим растојањима.

Укупна сила на честицу $i$ је:
\begin{equation}
\vec{F}_i = \sum_{j \neq i} \vec{F}_{ij}
\label{eq:total_force}
\end{equation}

\subsection{Временска интеграција}

За интеграцију једначина кретања користи се Ојлеров метод првог реда (\textit{Forward Euler}):

\begin{align}
\vec{v}_{i}^{n+1} &= \vec{v}_{i}^{n} + \Delta t \frac{\vec{F}_i^n}{m_i} \\
\vec{r}_{i}^{n+1} &= \vec{r}_{i}^{n} + \Delta t \cdot \vec{v}_{i}^{n+1}
\end{align}

Ова метода је једноставна за имплементацију, али нумерички мање стабилна од симплектичких метода попут \textit{leapfrog} или \textit{velocity Verlet} интегратора. За ову студију, избор интегратора није критичан јер је фокус на поређењу архитектонских перформанси различитих формулација и платформи, а не на дугорочној прецизности симулације.

\subsection{Рачунска сложеност}

Директна метода захтева израчунавање $N(N-1)/2$ јединствених парова интеракција (симетрична) или $N(N-1)$ интеракција (несиметрична). У оба случаја, временска сложеност је $O(N^2)$ по кораку симулације.

За симулацију од $T$ временских корака, укупна сложеност је $O(T \cdot N^2)$. При $N = 12000$ честица, сваки временски корак захтева преко 143 милиона интеракција (несиметрична формулација).


% ============================================================================
% ОПИС АЛГОРИТМА
% ============================================================================
\section{Опис алгоритма}

\subsection{Несиметрична формулација}

У несиметричној формулацији, свака честица $i$ итерира кроз све остале честице $j \neq i$ и акумулира силу:

\begin{verbatim}
for i = 0 to N-1:
    F[i] = 0
    for j = 0 to N-1:
        if i != j:
            F[i] += compute_force(i, j)
\end{verbatim}

Главна предност несиметричне формулације је једноставна паралелизација: свака нит обрађује подскуп честица независно и нема потребе за синхронизацијом приликом ажурирања сила. Недостатак је редундантно рачунање — сила $\vec{F}_{ij}$ и $\vec{F}_{ji}$ се рачунају одвојено.

\subsection{Симетрична формулација}

Симетрична формулација користи Њутнов трећи закон:

\begin{verbatim}
for i = 0 to N-1:
    for j = i+1 to N-1:
        f = compute_force(i, j)
        F[i] += f
        F[j] -= f    // Newton III
\end{verbatim}

Главна предност симетричне формулације је преполовљен број рачунања сила: $(N^2 - N)/2$ уместо $N^2 - N$. Недостатак је што ажурирање \texttt{F[j]} уноси зависност података, па при паралелизацији захтева атомске операције или другу форму синхронизације.


% ============================================================================
% ДЕТАЉИ ИМПЛЕМЕНТАЦИЈЕ
% ============================================================================
\section{Детаљи имплементације}

\subsection{Секвенцијална имплементација}

Обе формулације (симетрична и несиметрична) имплементиране су у C језику. Подаци о честицама чувају се у структурама са позицијом, брзином, масом и акумулатором силе. Коришћена је оптимизација компајлера са \texttt{-O3 -march=native} заставицама.

\subsection{OpenMP имплементација}

OpenMP паралелизација примењена је на спољашњу петљу по честицама:

\textbf{Несиметрична варијанта:}
\begin{verbatim}
#pragma omp parallel for schedule(dynamic)
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        if (i != j) F[i] += compute_force(i, j);
    }
}
\end{verbatim}

\textbf{Симетрична варијанта:}
\begin{verbatim}
#pragma omp parallel for schedule(dynamic)
for (int i = 0; i < N; i++) {
    for (int j = i+1; j < N; j++) {
        vec3 f = compute_force(i, j);
        F[i] += f;
        #pragma omp atomic
        F[j].x -= f.x;
        #pragma omp atomic
        F[j].y -= f.y;
        #pragma omp atomic
        F[j].z -= f.z;
    }
}
\end{verbatim}

Атомске операције уводе значајан \textit{overhead} због контенције на кеш линијама (\textit{false sharing}), серијализације приступа меморији и инвалидације кеша на другим језгрима \cite{chapman2008openmp}. OpenMP подешавања \texttt{OMP\_PLACES=cores} и \texttt{OMP\_PROC\_BIND=close} коришћена су за оптимално мапирање нити на језгра.

\subsection{CUDA имплементација}

CUDA имплементација развијена је у две варијанте:
\begin{enumerate}
    \item \textbf{Наивна (Non-Symmetric)}: свака нит израчунава силу на једну честицу читајући податке свих осталих честица директно из глобалне меморије.
    \item \textbf{Тајлована (Tiled)}: користи \textit{shared memory} за кеширање блокова честица, смањујући број приступа глобалној меморији.
\end{enumerate}

Наивна варијанта (коришћена у финалном поређењу):

\begin{verbatim}
__global__ void compute_forces_naive(
    Vec3 *pos, double *mass, Vec3 *acc, int n)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= n) return;
    
    double ax = 0, ay = 0, az = 0;
    Vec3 pi = pos[i];
    
    for (int j = 0; j < n; j++) {
        Vec3 pj = pos[j];
        double dx = pj.x - pi.x;
        double dy = pj.y - pi.y;
        double dz = pj.z - pi.z;
        double dist_sq = dx*dx + dy*dy + dz*dz + SOFTENING;
        double inv_dist3 = 1.0 / (dist_sq * sqrt(dist_sq));
        double f = mass[j] * inv_dist3;
        ax += f * dx;  ay += f * dy;  az += f * dz;
    }
    acc[i] = {ax, ay, az};
}
\end{verbatim}

Тајлована варијанта користи \textit{shared memory} за смањење латенције:

\begin{verbatim}
__global__ void compute_forces_tiled(
    Vec3 *pos, double *mass, Vec3 *acc, int n)
{
    extern __shared__ double shared_mem[];
    // Кеширање позиција и маса у shared memory
    // по блоковима од TILE_SIZE честица
    for (tile = 0; tile < num_tiles; tile++) {
        // Учитај блок у shared memory
        __syncthreads();
        // Рачунај силе из кешираног блока
        __syncthreads();
    }
}
\end{verbatim}

\textbf{Поређење варијанти}: Експериментално је утврђено да тајлована варијанта не показује значајно убрзање у односу на наивну (speedup 0,94×--1,04×, медијана $\sim$0,99×). Модерни GPU-ови базирани на Ada Lovelace архитектури поседују велике L2 кешеве (32 MB на RTX 4070 Laptop) који ефикасно кеширају глобалне приступе \cite{nvidia2023cuda}. За проблеме величине $N \leq 12000$, подаци о честицама ($\sim$300 KB) у потпуности стају у L2 кеш, чинећи експлицитно коришћење shared memory редундантним. Додатно, \textit{overhead} синхронизације (\texttt{\_\_syncthreads()}) умањује евентуални добитак од смањене латенције. Из ових разлога, за финално поређење изабрана је \textbf{наивна варијанта} због једноставности и еквивалентних перформанси.

\subsection{Docker окружење за CUDA}

Због проблема са компатибилношћу CUDA Toolkit верзија на хост систему (Fedora 43 са GCC 15.2.1 није компатибилан са CUDA 12.2), CUDA бенчмарци су извршени у Docker контејнеру:

\begin{verbatim}
FROM nvidia/cuda:12.2.0-devel-ubuntu22.04
# GCC 11 компатибилан са CUDA 12.2
RUN apt-get install -y build-essential
\end{verbatim}

Контејнер користи NVIDIA Container Toolkit за директан приступ GPU хардверу, обезбеђујући конзистентно окружење за репродуцибилне резултате.

OpenMP подешавања: \texttt{OMP\_PLACES=cores}, \texttt{OMP\_PROC\_BIND=close}.


% ============================================================================
% ЕКСПЕРИМЕНТАЛНА ПОСТАВКА
% ============================================================================
\section{Експериментална поставка}

\subsection{Хардверска конфигурација}

Експерименти су извршени на следећем систему:

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Компонента} & \textbf{Спецификација} \\ \midrule
CPU & Intel Core i9-13900HX \\
Језгра/Нити & 24 језгра / 32 логичке нити \\
L3 кеш & 36 MiB \\
RAM & DDR5 \\
GPU & NVIDIA GeForce RTX 4070 Laptop \\
GPU меморија & 8 GB GDDR6 \\
CUDA Compute Capability & 8.9 \\ \bottomrule
\end{tabular}
\caption{Хардверска конфигурација тест система}
\label{table:hardware}
\end{table}

\subsection{Софтверска конфигурација}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Софтвер} & \textbf{Верзија} \\ \midrule
Оперативни систем & Fedora 43 (Linux 6.17.12) \\
GCC компајлер & 15.2.1 \\
CUDA Toolkit & 12.2 \\
NVIDIA драјвер & 580.119.02 \\ \bottomrule
\end{tabular}
\caption{Софтверска конфигурација}
\label{table:software}
\end{table}

\subsection{Методологија мерења}

Свака конфигурација извршена је 5 пута независно, а као метрика коришћена је медијана времена извршавања која је робуснија од средње вредности у присуству outlier-а. Пре мерења извршено је једно загревање ради елиминације ефеката хладног кеша и JIT компилације. За CPU мерење времена коришћена је функција \texttt{clock\_gettime()} са \texttt{CLOCK\_MONOTONIC}, док су за GPU коришћени CUDA events који прецизно мере време извршавања кернела \cite{nvidia2023cuda}.

Спроведена су три типа експеримената: (1) \textbf{strong scaling} са фиксним $N = 6000$ и варирањем броја нити од 1 до 32, (2) \textbf{size sweep} са варирањем $N$ од 1000 до 12000 при фиксних 32 нити, и (3) \textbf{финално поређење} најбољих варијанти CPU и GPU имплементација.


% ============================================================================
% РЕЗУЛТАТИ
% ============================================================================
\section{Резултати}

\subsection{Поређење формулација: секвенцијална имплементација}

Табела~\ref{table:seq_formulation} приказује времена извршавања секвенцијалних имплементација за различите величине проблема.

\begin{table}[H]
\centering
\begin{tabular}{@{}rrrr@{}}
\toprule
$N$ & Несиметрична [s] & Симетрична [s] & Убрзање \\ \midrule
1000 & 0,0501 & 0,0355 & 1,41× \\
2000 & 0,2018 & 0,1419 & 1,42× \\
4000 & 0,8012 & 0,5672 & 1,41× \\
6000 & 1,7968 & 1,2759 & 1,41× \\
8000 & 3,1988 & 2,2680 & 1,41× \\
10000 & 4,9899 & 3,5475 & 1,41× \\
12000 & 8,8045 & 6,2467 & 1,41× \\ \bottomrule
\end{tabular}
\caption{Поређење секвенцијалних формулација}
\label{table:seq_formulation}
\end{table}

Симетрична формулација конзистентно остварује убрзање од приближно 1,4× у односу на несиметричну, што одговара преполовљеном броју рачунања сила. Слика~\ref{fig:formulation} визуелно приказује ово поређење.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{formulation_comparison.pdf}
    \caption{Поређење секвенцијалних формулација: симетрична vs. несиметрична}
    \label{fig:formulation}
\end{figure}

\subsection{Поређење формулација: OpenMP имплементација}

За разлику од секвенцијалне имплементације, код OpenMP симетрична формулација показује значајно лошије перформансе (Табела~\ref{table:omp_formulation}).

\begin{table}[H]
\centering
\begin{tabular}{@{}rrrr@{}}
\toprule
$N$ & Несиметрична [s] & Симетрична [s] & Фактор успорења \\ \midrule
1000 & 0,0109 & 0,0936 & 8,6× \\
2000 & 0,0287 & 0,2847 & 9,9× \\
4000 & 0,0848 & 0,8550 & 10,1× \\
6000 & 0,1789 & 1,6463 & 9,2× \\
8000 & 0,3136 & 2,6322 & 8,4× \\
10000 & 0,4768 & 3,7855 & 7,9× \\
12000 & 0,6870 & 5,1475 & 7,5× \\ \bottomrule
\end{tabular}
\caption{OpenMP формулације (32 нити): симетрична је 7,5--10× спорија}
\label{table:omp_formulation}
\end{table}

Овај неочекивани резултат последица је атомских операција потребних за ажурирање акумулатора силе честице $j$. Слика~\ref{fig:omp_formulation} илуструје ову разлику.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{omp_formulation.pdf}
    \caption{OpenMP формулације (32 нити): несиметрична значајно надмашује симетричну}
    \label{fig:omp_formulation}
\end{figure}

\subsection{Strong Scaling: OpenMP}

Слика~\ref{fig:strong_scaling} приказује скалирање OpenMP имплементација са бројем нити за $N = 6000$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{strong_scaling.pdf}
    \caption{Strong scaling OpenMP имплементација ($N = 6000$)}
    \label{fig:strong_scaling}
\end{figure}

Несиметрична формулација показује добро скалирање до 24 нити, након чега долази до засићења услед два фактора: хипертрединга (32 логичке нити на 24 физичка језгра, где два хипертреда деле исте извршне јединице) и ограничења меморијског пропусног опсега. Симетрична формулација показује \textit{негативно скалирање} — перформансе се погоршавају са повећањем броја нити због контенције атомских операција, што је добро документовано у литератури о OpenMP паралелизацији \cite{chapman2008openmp}.

Табела~\ref{table:strong_scaling} приказује нумеричке вредности за несиметричну формулацију:

\begin{table}[H]
\centering
\begin{tabular}{@{}rrrrr@{}}
\toprule
Нити & Време [s] & Убрзање & Ефикасност \\ \midrule
1 & 1,8137 & 0,70× & 70,3\% \\
2 & 1,1143 & 1,14× & 57,2\% \\
4 & 0,5594 & 2,28× & 57,0\% \\
8 & 0,2918 & 4,37× & 54,6\% \\
16 & 0,3425 & 3,72× & 23,3\% \\
24 & 0,2334 & 5,47× & 22,8\% \\
32 & 0,1785 & 7,15× & 22,3\% \\ \bottomrule
\end{tabular}
\caption{Strong scaling OpenMP несиметричне формулације ($N = 6000$)}
\label{table:strong_scaling}
\end{table}

Убрзање је рачунато у односу на секвенцијалну симетричну имплементацију (најбржу секвенцијалну).

\subsection{Финално поређење: CPU vs. GPU}

За финално поређење изабране су три имплементације: \textbf{CPU секвенцијална} (симетрична формулација као најбржа секвенцијална), \textbf{CPU OpenMP} (несиметрична формулација са 32 нити као најбржа паралелна) и \textbf{GPU CUDA} (наивна несиметрична формулација).

\begin{table}[H]
\centering
\begin{tabular}{@{}rrrrrr@{}}
\toprule
$N$ & Секв. [s] & OpenMP [s] & CUDA [s] & Убрзање OMP & Убрзање CUDA \\ \midrule
1000 & 0,0355 & 0,0109 & 0,0168 & 3,25× & 2,11× \\
2000 & 0,1419 & 0,0287 & 0,0333 & 4,94× & 4,27× \\
4000 & 0,5672 & 0,0848 & 0,0584 & 6,69× & 9,71× \\
6000 & 1,2759 & 0,1789 & 0,0864 & 7,13× & 14,77× \\
8000 & 2,2680 & 0,3136 & 0,1192 & 7,23× & 19,02× \\
10000 & 3,5475 & 0,4768 & 0,2982 & 7,44× & 11,90× \\
12000 & 6,2467 & 0,6870 & 0,3541 & 9,09× & 17,64× \\ \bottomrule
\end{tabular}
\caption{Финално поређење имплементација}
\label{table:final_comparison}
\end{table}

Слика~\ref{fig:final_comparison} визуелно приказује ово поређење.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{final_comparison.pdf}
    \caption{Финално поређење: секвенцијално vs. OpenMP vs. CUDA}
    \label{fig:final_comparison}
\end{figure}

Анализа резултата открива три кључна налаза. За мале проблеме ($N \leq 2000$), OpenMP надмашује CUDA због \textit{overhead}-а трансфера података између CPU и GPU меморије и латенције покретања кернела. Тачка прелома налази се при $N \approx 4000$, где CUDA преузима доминацију захваљујући масивној паралелности GPU-а \cite{kirk2016programming}. При $N = 12000$, CUDA остварује убрзање од 17,6× у односу на секвенцијалну имплементацију.


% ============================================================================
% ДИСКУСИЈА
% ============================================================================
\section{Дискусија}

\subsection{Утицај формулације на секвенцијалне перформансе}

Симетрична формулација преполовљује број рачунања сила искоришћавањем Њутновог трећег закона. Очекивано убрзање од 2× није достигнуто (измерено $\sim$1,4×) из неколико разлога. Прво, симетрична формулација захтева додатне меморијске приступе за ажурирање акумулатора \texttt{F[j]}. Друго, меморијски приступ постаје мање регуларан јер унутрашња петља почиње од \texttt{j = i+1} уместо од 0, што смањује ефикасност хардверског префетчера. Треће, ажурирање два акумулатора по интеракцији смањује ефикасност кеша.

\subsection{Зашто симетрија штети OpenMP имплементацији}

Симетрична OpenMP имплементација захтева атомске операције за ажурирање \texttt{F[j]}, што уводи неколико проблема:

\begin{enumerate}
    \item \textbf{Контенција}: више нити истовремено покушава да ажурира исту локацију.
    \item \textbf{False sharing}: честице у близини деле исту кеш линију.
    \item \textbf{Кеш инвалидација}: свака атомска операција инвалидира кеш линију на другим језгрима.
    \item \textbf{Серијализација}: атомске операције ефективно серијализују извршавање.
\end{enumerate}

Са 32 нити, контенција постаје доминантан фактор, и симетрична формулација постаје 7--10× спорија од несиметричне.

\subsection{GPU предности за велике проблеме}

CUDA имплементација надмашује CPU за $N \geq 4000$ захваљујући неколико архитектонских предности GPU-а. RTX 4070 Laptop поседује 4608 CUDA језгара која омогућавају масивну паралелност, знатно превазилазећи 32 логичке нити CPU-а \cite{nyland2007fast}. GPU меморија (GDDR6) остварује пропусни опсег од 256 GB/s, што је вишеструко више од DDR5 меморије. Додатно, GPU архитектура ефикасно скрива меморијску латенцију кроз велики број активних warp-ова који се наизменично извршавају док други чекају на податке \cite{kirk2016programming}. Коначно, N-body проблем је рачунски интензиван (\textit{compute-bound}), што идеално одговара GPU архитектури оптимизованој за throughput.

За мале проблеме, \textit{overhead} преноса података између CPU и GPU меморије и латенција покретања кернела доминирају над корисним рачунањем, што објашњава зашто OpenMP побеђује за $N \leq 2000$.

\subsection{Ограничења студије}

Ова студија има неколико ограничења која треба узети у обзир при интерпретацији резултата. Коришћена је наивна $O(N^2)$ метода уместо ефикаснијих алгоритама попут Barnes--Hut \cite{barnes1986hierarchical}, што ограничава практичну применљивост за велике системе. Тестирања су спроведена само на једном GPU моделу (RTX 4070 Laptop), па резултати не могу бити директно генерализовани на друге GPU архитектуре. Такође, NUMA ефекти на вишесокетним системима нису детаљно анализирани.


% ============================================================================
% ЗАКЉУЧАК
% ============================================================================
\section{Закључак}

Овај рад је представио детаљну анализу паралелизације N-body симулације са квадратном сложеношћу. Кључни закључци су:

\begin{enumerate}
    \item \textbf{Симетрична формулација} убрзава секвенцијално извршавање за ~1,4×, али \textbf{драматично успорава} OpenMP имплементацију (7--10×) због контенције атомских операција.
    
    \item \textbf{OpenMP несиметрична} имплементација постиже убрзање до 9× на 32 нити, са паралелном ефикасношћу од ~22\%.
    
    \item \textbf{CUDA имплементација} надмашује CPU за $N \geq 4000$ и постиже убрзање од 17,6× при $N = 12000$.
    
    \item Избор формулације мора узети у обзир циљну архитектуру — оптимална секвенцијална формулација није нужно оптимална за паралелно извршавање.
\end{enumerate}

\subsection{Будући рад}

Могући правци даљег истраживања укључују имплементацију Barnes--Hut алгоритма \cite{barnes1986hierarchical} са сложеношћу $O(N \log N)$ за симулацију већих система са милионима честица. За дистрибуиране системе била би интересантна хибридна MPI+CUDA имплементација која комбинује међучворну комуникацију са GPU акцелерацијом. Мулти-GPU имплементација са NVLink интерконектом могла би додатно скалирати перформансе. Коначно, детаљнија анализа меморијског пропусног опсега помоћу roofline модела и профилисања кеш понашања пружила би дубљи увид у перформансна ограничења.


% ============================================================================
% ЛИТЕРАТУРА
% ============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{aarseth2003gravitational}
S.~J. Aarseth.
\newblock {\em Gravitational N-Body Simulations: Tools and Algorithms}.
\newblock Cambridge University Press, 2003.

\bibitem{barnes1986hierarchical}
J.~Barnes and P.~Hut.
\newblock A hierarchical {O(N log N)} force-calculation algorithm.
\newblock {\em Nature}, 324(6096):446--449, 1986.

\bibitem{greengard1987fast}
L.~Greengard and V.~Rokhlin.
\newblock A fast algorithm for particle simulations.
\newblock {\em Journal of Computational Physics}, 73(2):325--348, 1987.

\bibitem{nvidia2023cuda}
NVIDIA Corporation.
\newblock {\em CUDA C++ Programming Guide}, version 12.2.
\newblock \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}, 2023.

\bibitem{chapman2008openmp}
B.~Chapman, G.~Jost, and R.~van~der Pas.
\newblock {\em Using OpenMP: Portable Shared Memory Parallel Programming}.
\newblock MIT Press, 2008.

\bibitem{kirk2016programming}
D.~B. Kirk and W.~W. Hwu.
\newblock {\em Programming Massively Parallel Processors: A Hands-on Approach}.
\newblock Morgan Kaufmann, 3rd edition, 2016.

\bibitem{nyland2007fast}
L.~Nyland, M.~Harris, and J.~Prins.
\newblock Fast N-body simulation with CUDA.
\newblock In {\em GPU Gems 3}, chapter 31.
\newblock Addison-Wesley, 2007.

\end{thebibliography}

\end{document}
